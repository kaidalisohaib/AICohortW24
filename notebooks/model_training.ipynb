{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kaidalisohaib/AICohortW24/blob/Sohaib/notebooks/model_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "waifusAK8aT7",
        "outputId": "97bd1571-bd13-4138-d5bb-b8dde404c960"
      },
      "outputs": [],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AM4tSWaW8aT9"
      },
      "source": [
        "## Step 1: Import necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BtnHVdAw8aUB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import logging\n",
        "from math import ceil\n",
        "from datetime import datetime\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
        "from sklearn.preprocessing import MinMaxScaler, PolynomialFeatures\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, average_precision_score, make_scorer, precision_score, balanced_accuracy_score, precision_recall_curve\n",
        "from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE, SVMSMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler, TomekLinks, EditedNearestNeighbours\n",
        "from joblib import Memory\n",
        "from sklearn.pipeline import Pipeline\n",
        "import onnx\n",
        "from onnx import shape_inference\n",
        "from skl2onnx import convert_sklearn\n",
        "from skl2onnx.common.data_types import FloatTensorType\n",
        "from onnxruntime import InferenceSession\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "sns.set_context(\"notebook\")\n",
        "sns.set_theme(style=\"ticks\")\n",
        "sns.color_palette(\"rocket\", as_cmap=True)\n",
        "\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Create a memory object to cache the results\n",
        "memory = Memory(location='cache', verbose=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PG5lBZS-8aUC"
      },
      "source": [
        "## Step 2: Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnTXIfbS8aUC",
        "outputId": "cf8bde15-a588-4a90-a551-73e5f45d70ed"
      },
      "outputs": [],
      "source": [
        "# input_file_path = \"..\\\\data\\\\heart_disease_health_indicators_BRFSS2015.csv\"\n",
        "input_file_path = \"https://raw.githubusercontent.com/kaidalisohaib/AICohortW24/main/data/heart_disease_health_indicators_BRFSS2015.csv\"\n",
        "\n",
        "df = pd.read_csv(input_file_path, header = 0)\n",
        "\n",
        "df.head()\n",
        "\n",
        "all_columns = df.columns.tolist()\n",
        "\n",
        "# Define target column\n",
        "target_column = \"HeartDiseaseorAttack\"\n",
        "\n",
        "# Get all feature columns except one\n",
        "all_features = all_columns\n",
        "all_features.remove(target_column)\n",
        "\n",
        "# Get numerical columns\n",
        "numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "\n",
        "# Get non-numerical (categorical) columns\n",
        "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "print(\"All the columns:\\n\", all_columns)\n",
        "print(\"Target column:\\n\", target_column)\n",
        "print(\"Features columns:\\n\", all_features)\n",
        "print(\"Numerical Columns:\\n\",numerical_cols)\n",
        "print(\"Categorical Columns:\\n\",categorical_cols)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSN_rEdQ8aUC"
      },
      "source": [
        "## Step 3: Data exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6oKjZZNyRGW"
      },
      "source": [
        "### 3.1 Descriptive Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nruZdScbyRGW",
        "outputId": "9046ca96-251a-4554-de68-98e8e8ff9f6a"
      },
      "outputs": [],
      "source": [
        "# Print the first few rows of the dataset\n",
        "print(\"First few rows of the dataset:\")\n",
        "print(df.head())\n",
        "\n",
        "# Get summary statistics of numerical columns\n",
        "print(\"\\nSummary statistics of numerical columns:\")\n",
        "print(df.describe())\n",
        "\n",
        "# Get the count of unique values in each column\n",
        "print(\"\\nCount of unique values in each column:\")\n",
        "print(df.nunique())\n",
        "\n",
        "print(\"\\nCount of missing values:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Get the count of duplicate rows\n",
        "print(\"\\nCount of duplicated rows:\")\n",
        "print(df.duplicated(keep=False).sum())\n",
        "\n",
        "# Value counts of a numerical variable\n",
        "print(\"\\nValue counts of a numerical variable:\")\n",
        "print(len(numerical_cols))\n",
        "\n",
        "# Value counts of a categorical variable\n",
        "print(\"\\nValue counts of a categorical variable:\")\n",
        "print(len(categorical_cols))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmTrWzQeyRGW"
      },
      "source": [
        "### 3.2 Data Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PF--CWnkyRGX",
        "outputId": "ab67633a-1152-4f31-ae3a-8a7e3fcc7d4b"
      },
      "outputs": [],
      "source": [
        "sample_df = df.sample(1_000, random_state=42)\n",
        "\n",
        "df.hist(figsize=(20,20))\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "###########\n",
        "\n",
        "# Define a function to create the pairplot\n",
        "def create_pairplot(df, all_features, target_column):\n",
        "    # Determine the number of columns in the pairplot grid\n",
        "    num_cols = 5  # Adjust the number of columns as needed\n",
        "\n",
        "    # Calculate the number of rows needed for the pairplot grid\n",
        "    num_rows = (len(all_features) + num_cols - 1) // num_cols\n",
        "\n",
        "    # Create a single figure and axes for the pairplot grid\n",
        "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(5*num_cols, 5*num_rows))\n",
        "\n",
        "    # Plot each feature against the target variable\n",
        "    for i, feature in enumerate(all_features):\n",
        "        row = i // num_cols\n",
        "        col = i % num_cols\n",
        "        ax = axes[row, col]\n",
        "        sns.kdeplot(data=df, x=feature, hue=target_column, fill=True, ax=ax, warn_singular=False)\n",
        "        ax.set_ylabel(\"Frequency\")\n",
        "\n",
        "    # Hide empty subplots\n",
        "    for i in range(len(all_features), num_rows*num_cols):\n",
        "        row = i // num_cols\n",
        "        col = i % num_cols\n",
        "        fig.delaxes(axes[row, col])\n",
        "\n",
        "    # Show the combined pairplot image\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "# Call the function to create the pairplot\n",
        "pairplot_fig = create_pairplot(sample_df, all_features, target_column)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfXjUNDRyRGX"
      },
      "source": [
        "### 3.3 Pattern identification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 874
        },
        "id": "JCXs2pPb8aUC",
        "outputId": "34df6705-9584-400f-9879-bb8efee10534"
      },
      "outputs": [],
      "source": [
        "# plotting data on chart\n",
        "# plt.pie(sample_df[target_column].value_counts(), )\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 8))\n",
        "\n",
        "_ = sample_df[target_column].value_counts().plot(kind='pie', autopct='%1.1f%%', ax=ax, labels=[\"Does not have HeartDiseaseorAttack\", \"Have HeartDiseaseorAttack\"])\n",
        "_ = ax.yaxis.set_visible(False)\n",
        "# displaying chart\n",
        "plt.show()\n",
        "\n",
        "# Correlation matrix\n",
        "plt.figure(figsize=(15,12))\n",
        "sns.heatmap(df.corr(), annot=True, fmt=\".2f\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfPUCfx38aUD"
      },
      "source": [
        "## Step 4: Data preprocessing\n",
        "TODO: Handle missing values, encode categorical variables, perform feature scaling, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQ82UOYZ8aUD"
      },
      "outputs": [],
      "source": [
        "df.drop_duplicates(keep='first', inplace=True)\n",
        "# No missing value so we're good"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLuYRSOj8aUE"
      },
      "source": [
        "## Step 6: Split the dataset into train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ln6LnEYJ8aUE"
      },
      "outputs": [],
      "source": [
        "# Sample the dataset\n",
        "def sample_dataset(df, frac=0.1, random_state=42):\n",
        "    logger.info(\"Sampling the dataset...\")\n",
        "    return df.sample(1000, random_state=random_state)\n",
        "\n",
        "# Split data into train and test sets\n",
        "def split_train_test(sample_df, all_features, target_column, test_size=0.2, random_state=42):\n",
        "    logger.info(\"Splitting data into train and test sets...\")\n",
        "    X_train, X_test, y_train, y_test = train_test_split(sample_df[all_features], sample_df[target_column], test_size=test_size, random_state=random_state)\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "# Call the functions to sample the dataset and split train-test sets\n",
        "sample_df = sample_dataset(df)\n",
        "X_train, X_test, y_train, y_test = split_train_test(sample_df, all_features, target_column)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSGktgkB8aUE"
      },
      "source": [
        "## Step 7: Feature scaling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Feature scaling is included in the Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqTmb63Y8aUF"
      },
      "source": [
        "## Step 8: Model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKf-f2YV8aUF",
        "outputId": "eafe8b95-368f-4d66-b600-5a9a99ac5029"
      },
      "outputs": [],
      "source": [
        "# Define a function to create a scorer for PR AUC\n",
        "def pr_auc_scorer(y_true, y_pred):\n",
        "    return average_precision_score(y_true, y_pred)\n",
        "\n",
        "# Wrap the grid search function with the memory object\n",
        "@memory.cache\n",
        "def grid_search_fit(pipeline, param_grid, X_train, y_train):\n",
        "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
        "    grid_search = GridSearchCV(pipeline, param_grid, cv=cv, scoring=make_scorer(pr_auc_scorer, response_method=\"predict\"), verbose=3, n_jobs=-1)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    return grid_search\n",
        "\n",
        "# Define resampling techniques\n",
        "logger.info(\"Defining resampling techniques...\")\n",
        "resampling_techniques = {\n",
        "    'smote': SMOTE(random_state=42),\n",
        "    'adasyn': ADASYN(random_state=42),\n",
        "    'borderline_smote': BorderlineSMOTE(random_state=42),\n",
        "    'svm_smote': SVMSMOTE(random_state=42),\n",
        "    'random_under_sampler': RandomUnderSampler(random_state=42),\n",
        "    'tomek_links': TomekLinks(),\n",
        "    'edited_nearest_neighbours': EditedNearestNeighbours()\n",
        "}\n",
        "\n",
        "# Define classifiers\n",
        "logger.info(\"Defining classifiers...\")\n",
        "classifiers = {\n",
        "    'rf': RandomForestClassifier(random_state=42, class_weight='balanced'),\n",
        "    'gb': GradientBoostingClassifier(random_state=42)\n",
        "}\n",
        "\n",
        "# Define pipelines for different strategies\n",
        "logger.info(\"Creating pipelines...\")\n",
        "pipelines = {}\n",
        "for resampling_name, resampling_technique in resampling_techniques.items():\n",
        "    for classifier_name, classifier in classifiers.items():\n",
        "        pipe_name = f\"{resampling_name}_{classifier_name}\"\n",
        "        scaler = MinMaxScaler()\n",
        "        poly_features = PolynomialFeatures(degree=2)  # Adjust degree as needed\n",
        "        pipeline = Pipeline([\n",
        "            ('scaler', scaler),\n",
        "            ('poly_features', poly_features),  # Add PolynomialFeatures\n",
        "            ('classifier', classifier)\n",
        "        ])\n",
        "        pipelines[pipe_name] = (pipeline, resampling_technique)\n",
        "\n",
        "# Define parameter grids for grid search\n",
        "logger.info(\"Defining parameter grids for grid search...\")\n",
        "param_grids = {\n",
        "    'rf': {\n",
        "        'classifier__n_estimators': [100, 200],\n",
        "        'classifier__max_depth': [10, 20]\n",
        "    },\n",
        "    'gb': {\n",
        "        'classifier__n_estimators': [50, 100],\n",
        "        'classifier__learning_rate': [0.01, 0.1]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Perform grid search and evaluate models\n",
        "logger.info(\"Performing grid search and evaluating models...\")\n",
        "results = {}\n",
        "pipeline_total = len(pipelines)\n",
        "start = time.time()\n",
        "for idx, (name, (pipeline, resampling_technique)) in enumerate(pipelines.items()):\n",
        "    logger.info(f\"Training {name} {100*idx/pipeline_total:.1f}%...\")\n",
        "    classifier_name = name.split('_')[-1]\n",
        "    param_grid = param_grids[classifier_name]\n",
        "    X_train_resampled, y_train_resampled = resampling_technique.fit_resample(X_train, y_train)\n",
        "    # Call the grid search function, which will check the cache before running\n",
        "    grid_search = grid_search_fit(pipeline, param_grid, X_train_resampled, y_train_resampled)\n",
        "    best_model = grid_search.best_estimator_\n",
        "    y_pred = best_model.predict(X_test)\n",
        "    report = classification_report(y_test, y_pred)\n",
        "    results[name] = {\n",
        "        'best_model': best_model,\n",
        "        'classification_report': report\n",
        "    }\n",
        "\n",
        "elapsed_time = time.time() - start\n",
        "\n",
        "# Convert elapsed time to hours, minutes, and seconds\n",
        "hours, remainder = divmod(elapsed_time, 3600)\n",
        "minutes, seconds = divmod(remainder, 60)\n",
        "\n",
        "# Print elapsed time in a nice format\n",
        "print(f\"Total Training Time: {int(hours):02d} hours, {int(minutes):02d} minutes, {seconds:.2f} seconds\")\n",
        "\n",
        "# Print results\n",
        "logger.info(\"Printing results...\")\n",
        "for name, result in results.items():\n",
        "    print(f\"\\n{name.upper()} MODEL:\")\n",
        "    print(\"Best Model Parameters:\", result['best_model'].get_params())\n",
        "    print(\"Classification Report:\")\n",
        "    print(result['classification_report'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Model evaluation\n",
        "\n",
        "For our binary classification challenge PR AUC Score is more important"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "AORREMT0_18G",
        "outputId": "076207fa-aa39-41d1-f5bf-6c5883f63057"
      },
      "outputs": [],
      "source": [
        "# Define a function to evaluate models and collect scores\n",
        "def evaluate_models(results, X_test, y_test):\n",
        "    model_scores = []\n",
        "    for name, result in results.items():\n",
        "        pr_auc = average_precision_score(y_test, result['best_model'].predict_proba(X_test)[:, 1])\n",
        "        balanced_accuracy = balanced_accuracy_score(y_test, result['best_model'].predict(X_test))\n",
        "        report_dict = classification_report(y_test, result['best_model'].predict(X_test), output_dict=True)\n",
        "        accuracy = report_dict['accuracy']\n",
        "        recall = report_dict['1.0']['recall'] if '1.0' in report_dict else 0  # Recall for positive class\n",
        "        # F1-score for positive class\n",
        "        f1_score = report_dict['1.0']['f1-score'] if '1.0' in report_dict else 0\n",
        "        # Calculate precision for positive class\n",
        "        positive_precision = precision_score(y_test, result['best_model'].predict(X_test), pos_label=1)\n",
        "        # Append the scores to the list\n",
        "        model_scores.append((name, accuracy, recall, f1_score, pr_auc, balanced_accuracy, positive_precision))\n",
        "\n",
        "    # Sort the models based on each metric score in increasing order\n",
        "    model_scores_accuracy = sorted(model_scores, key=lambda x: x[1])\n",
        "    model_scores_recall = sorted(model_scores, key=lambda x: x[2])\n",
        "    model_scores_f1_score = sorted(model_scores, key=lambda x: x[3])\n",
        "    model_scores_pr_auc = sorted(model_scores, key=lambda x: x[4])\n",
        "    model_scores_balanced_accuracy = sorted(model_scores, key=lambda x: x[5])\n",
        "\n",
        "    # Extract model names and corresponding scores for plotting\n",
        "    model_names_accuracy = [name for name, _, _, _, _, _, _ in model_scores_accuracy]\n",
        "    model_names_recall = [name for name, _, _, _, _, _, _ in model_scores_recall]\n",
        "    model_names_f1_score = [name for name, _, _, _, _, _, _ in model_scores_f1_score]\n",
        "    model_names_pr_auc = [name for name, _, _, _, _, _, _ in model_scores_pr_auc]\n",
        "    model_names_balanced_accuracy = [name for name, _, _, _, _, _, _ in model_scores_balanced_accuracy]\n",
        "\n",
        "    accuracy_scores = [accuracy for _, accuracy, _, _, _, _, _ in model_scores_accuracy]\n",
        "    recall_scores = [recall for _, _, recall, _, _, _, _ in model_scores_recall]\n",
        "    f1_scores = [f1_score for _, _, _, f1_score, _, _, _ in model_scores_f1_score]\n",
        "    pr_auc_scores = [pr_auc for _, _, _, _, pr_auc, _, _ in model_scores_pr_auc]\n",
        "    balanced_accuracy_scores = [balanced_precision for _, _, _, _, _, balanced_precision, _ in model_scores_balanced_accuracy]\n",
        "    return (model_scores, model_names_accuracy, model_names_recall, model_names_f1_score, model_names_pr_auc, model_names_balanced_accuracy,\n",
        "            accuracy_scores, recall_scores, f1_scores, pr_auc_scores, balanced_accuracy_scores)\n",
        "\n",
        "# Call the function to evaluate models and collect scores\n",
        "(model_scores, model_names_accuracy, model_names_recall, model_names_f1_score, model_names_pr_auc, model_names_balanced_accuracy,\n",
        " accuracy_scores, recall_scores, f1_scores, pr_auc_scores, balanced_accuracy_scores) = evaluate_models(results, X_test, y_test)\n",
        "    \n",
        "# Plot scores\n",
        "plt.figure(figsize=(25, 20))\n",
        "\n",
        "# Accuracy\n",
        "plt.subplot(3, 2, 1)\n",
        "plt.plot(model_names_accuracy, accuracy_scores, label='Accuracy', marker='o')\n",
        "plt.xlabel('Model')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy for Each Model (Increasing Order)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Recall\n",
        "plt.subplot(3, 2, 2)\n",
        "plt.plot(model_names_recall, recall_scores, label='Recall', marker='o')\n",
        "plt.xlabel('Model')\n",
        "plt.ylabel('Recall')\n",
        "plt.title('Recall for Each Model (Increasing Order)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# F1-score\n",
        "plt.subplot(3, 2, 3)\n",
        "plt.plot(model_names_f1_score, f1_scores, label='F1-score', marker='o')\n",
        "plt.xlabel('Model')\n",
        "plt.ylabel('F1-score')\n",
        "plt.title('F1-score for Each Model (Increasing Order)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# PR AUC Score\n",
        "plt.subplot(3, 2, 4)\n",
        "plt.plot(model_names_pr_auc, pr_auc_scores, label='PR AUC Score', marker='o')\n",
        "plt.xlabel('Model')\n",
        "plt.ylabel('PR AUC Score')\n",
        "plt.title('PR AUC Score for Each Model (Increasing Order)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Balanced Precision Score\n",
        "plt.subplot(3, 2, 5)\n",
        "plt.plot(model_names_balanced_accuracy, balanced_accuracy_scores, label='Balanced Accuracy Score', marker='o', color='orange')\n",
        "plt.xlabel('Model')\n",
        "plt.ylabel('Balanced Accuracy Score')\n",
        "plt.title('Balanced Accuracy Score for Each Model (Increasing Order)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assuming model_scores is a list of (model_name, accuracy, recall, f1_score, pr_auc, balanced_accuracy, positive_precision) tuples\n",
        "top_models = sorted(model_scores, key=lambda x: x[4], reverse=True)[:3]\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "for model_name, _, _, _, _, _, _ in top_models:\n",
        "    # Assuming model_dict contains the trained model objects\n",
        "    model =  results[model_name][\"best_model\"]\n",
        "    \n",
        "    # Predict probabilities on the test set\n",
        "    y_scores = model.predict_proba(X_test)[:, 1]\n",
        "    \n",
        "    # Compute precision and recall\n",
        "    precision, recall, _ = precision_recall_curve(y_test, y_scores)\n",
        "    \n",
        "    # Calculate average precision\n",
        "    avg_precision = average_precision_score(y_test, y_scores)\n",
        "    \n",
        "    # Plot PR curve\n",
        "    plt.plot(recall, precision, label=f'{model_name} (AP = {avg_precision:.2f})')\n",
        "\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 10: Confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the number of rows and columns for the subplots grid\n",
        "num_rows = 3\n",
        "num_cols = ceil(len(results)/num_rows)\n",
        "\n",
        "# Create a new figure with a specified size\n",
        "plt.figure(figsize=(25, 15))\n",
        "\n",
        "# Loop through each model and plot its confusion matrix\n",
        "for i, (name, result) in enumerate(results.items(), 1):\n",
        "    # Get the predicted labels from the model\n",
        "    y_pred = result['best_model'].predict(X_test)\n",
        "    \n",
        "    # Compute the confusion matrix\n",
        "    cm = confusion_matrix(y_test, y_pred, normalize=\"true\")\n",
        "    \n",
        "    # Create a new subplot for the current model\n",
        "    plt.subplot(num_rows, num_cols, i)\n",
        "    \n",
        "    # Plot the confusion matrix\n",
        "    sns.heatmap(cm, annot=True, cmap=\"Blues\", cbar=False)\n",
        "    \n",
        "    # Set the title and labels for the subplot\n",
        "    plt.title(f'Confusion Matrix - {name}')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.ylabel('True label')\n",
        "\n",
        "# Adjust the layout of subplots to prevent overlapping\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 11: Choose the best model\n",
        "\n",
        "Since PR AUC values are similar across models, balanced accuracy serves as a reliable alternative metric for evaluating and selecting the best-performing model, especially in scenarios with imbalanced datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QrWUe_ND8aUF"
      },
      "outputs": [],
      "source": [
        "# Assuming model_scores is a list of (model_name, accuracy, recall, f1_score, pr_auc, balanced_accuracy, positive_precision) tuples\n",
        "best_model_name = sorted(model_scores, key=lambda x: x[5], reverse=True)[0][0]\n",
        "best_model = results[best_model_name][\"best_model\"]\n",
        "print(best_model_name)\n",
        "\n",
        "print(\"Best Model based on Recall Score:\")\n",
        "print(f\"Model Name: {best_model_name}\")\n",
        "print(\"Classification report:\\n\", results[best_model_name][\"classification_report\"])\n",
        "print(f\"Model Parameters: {results[best_model_name]['best_model'].get_params()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 12: Save the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define initial types based on your input data\n",
        "initial_types = [('input', FloatTensorType([None, X_train.shape[1]]))]\n",
        "\n",
        "# Convert the best model to ONNX format with initial types\n",
        "onnx_model = convert_sklearn(best_model, best_model_name, initial_types=initial_types)\n",
        "\n",
        "# Infer shapes (optional but recommended)\n",
        "onnx_model = shape_inference.infer_shapes(onnx_model)\n",
        "\n",
        "# Get the current date and time\n",
        "current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "\n",
        "# Define the path to save the ONNX model with timestamp\n",
        "onnx_file_path = f'../models/{best_model_name.replace(\" \", \"_\").lower()}_{current_time}.onnx'\n",
        "\n",
        "# Save the ONNX model to a file\n",
        "onnx.save_model(onnx_model, onnx_file_path)\n",
        "\n",
        "print(\"ONNX model saved successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Load the ONNX model from the file\n",
        "# loaded_onnx_model = onnx.load(onnx_file_path)\n",
        "\n",
        "# print(\"ONNX model loaded successfully.\")\n",
        "target_value = 1\n",
        "X_test_single_row = np.array(df[df[target_column] == target_value].iloc[:, 1:].head(2000).values)\n",
        "\n",
        "# Create an ONNX inference session with the loaded model\n",
        "onnx_session = InferenceSession(onnx_file_path)\n",
        "\n",
        "# Convert the input data to a dictionary with the appropriate key\n",
        "# input_data = {'input': X_test_single_row.astype(np.float32)}\n",
        "input_data = {'input': X_test_single_row.astype(np.float32)}\n",
        "\n",
        "# Run inference with the ONNX model\n",
        "output = onnx_session.run(None, input_data)\n",
        "\n",
        "# Assuming you have only one output, you can access it like this\n",
        "output_data = output[0]\n",
        "print(sum(output_data==target_value)/len(output_data))\n",
        "# Now, you can use output_data for further processing or analysis"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
